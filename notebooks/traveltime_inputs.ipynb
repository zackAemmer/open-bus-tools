{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import gtfs_kit as gk\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from openbustools.traveltime import grid\n",
    "from openbustools import plotting, standardfeeds, spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folders = ['kcm_realtime', 'atb_realtime']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 10\n",
    "route_ids = {k:[] for k in data_folders}\n",
    "for folder_name in data_folders:\n",
    "    folder_path = Path(\"..\", \"data\", folder_name, \"processed\", \"analysis\")\n",
    "    folder_days = list(folder_path.glob(\"*.pkl\"))\n",
    "    n_days = len(folder_days)\n",
    "    n_samples = []\n",
    "    n_points = []\n",
    "    for file_name in random.sample(folder_days, sample_size):\n",
    "        data = pd.read_pickle(file_name)\n",
    "        n_samples.append(len(pd.unique(data['shingle_id'])))\n",
    "        n_points.append(len(data))\n",
    "        route_ids[folder_name].append(pd.unique(data['route_id']))\n",
    "    print(folder_name)\n",
    "    print(f\"{n_days} days\")\n",
    "    print(f\"{np.mean(n_points) = :_.0f}, {np.std(n_points) = :_.0f} points per day\")\n",
    "    print(f\"{np.mean(n_samples) = :_.0f}, {np.std(n_samples) = :_.0f} samples per day\")\n",
    "    print(f\"{np.mean(n_points) * n_days = :_.0f} points\")\n",
    "    print(f\"{np.mean(n_samples) * n_days = :_.0f} samples\")\n",
    "    unique_routes = np.unique(np.concatenate(route_ids[folder_name]))\n",
    "    print(np.random.choice(unique_routes, int(len(unique_routes)*.05), replace=False))\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder_name in data_folders:\n",
    "    folder_path = Path(\"..\", \"data\", folder_name, \"processed\", \"analysis\")\n",
    "    folder_days = list(folder_path.glob(\"*.pkl\"))\n",
    "    day_sample = pd.read_pickle(random.choice(folder_days))\n",
    "    shingle_sample = random.choice(day_sample.shingle_id)\n",
    "    shingle_sample = day_sample[day_sample['shingle_id'] == shingle_sample]\n",
    "    print(f\"{folder_name} Route ID: {shingle_sample['route_id'].iloc[0]}\")\n",
    "    plotting.formatted_shingle_scatterplot(shingle_sample)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "for folder_name in data_folders:\n",
    "    folder_path = Path(\"..\", \"data\", folder_name, \"processed\", \"analysis\")\n",
    "    folder_days = list(folder_path.glob(\"*.pkl\"))\n",
    "    day_sample = pd.read_pickle(folder_path / random.choice(folder_days)).to_crs(\"EPSG:4326\")\n",
    "    all_data.append(day_sample)\n",
    "all_data = pd.concat(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.formatted_feature_distributions_histplot(all_data, title_text=\"network_sample_distributions\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# importlib.reload(plotting)\n",
    "\n",
    "# cleaned_sources = pd.read_csv(\"../data/cleaned_sources.csv\")\n",
    "\n",
    "# for i,row in cleaned_sources.iloc[:].iterrows():\n",
    "#     try:\n",
    "#         # Load network realtime data from a random available day\n",
    "#         folder_name = f\"{row['uuid']}_realtime\"\n",
    "#         file_path = Path(\"..\", \"data\", \"other_feeds\", folder_name, \"processed\", \"grid\", \"2024_04_10.pkl\")\n",
    "#         print(f\"Processing {folder_name}\")\n",
    "#         day_sample = pickle.load(open(file_path, 'rb'))\n",
    "#         grid_frames = grid.convert_to_frames(day_sample)\n",
    "#         # grid_frames = grid.ffill_array(grid_frames)\n",
    "\n",
    "#         # Make the plot\n",
    "#         plotting.formatted_grid_animation(grid_frames, title_text=f\"grid_animation_{folder_name}\", start_frame=120, end_frame=900, location_str=f\"{row['provider']} - ({row['municipality']}, {row['country_code']})\", fps=20)\n",
    "#     except:\n",
    "#         print(f\"Error processing {folder_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Interpolate location and time\n",
    "# realtime_data['timestamp'] = pd.to_datetime(realtime_data['locationtime'], unit='s')\n",
    "# realtime_data['x_interp'] = realtime_data['x'].resample()\n",
    "# sns.histplot(all_realtime_data['calc_time_s'])\n",
    "# realtime_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(plotting)\n",
    "\n",
    "cleaned_sources = pd.read_csv(\"../data/cleaned_sources.csv\")\n",
    "\n",
    "all_realtime_data = []\n",
    "all_static_data = []\n",
    "for i,row in cleaned_sources.iloc[:].iterrows():\n",
    "    # Load network realtime data from a random available day\n",
    "    realtime_file_path = Path(\"..\", \"data\", \"other_feeds\", f\"{row['uuid']}_realtime\", \"processed\", \"analysis\")\n",
    "    realtime_files = list(realtime_file_path.glob(\"*.pkl\"))\n",
    "    if len(realtime_files) == 0:\n",
    "        print(f\"No realtime data found for {row['uuid']}\")\n",
    "        continue\n",
    "    file_path = random.choice(realtime_files)\n",
    "    print(f\"Processing {file_path}\")\n",
    "    realtime_data = pd.read_pickle(file_path).to_crs(\"EPSG:4326\")\n",
    "    # Load network static data corresponding to the realtime data\n",
    "    static_file_path = Path(\"..\", \"data\", \"other_feeds\", f\"{row['uuid']}_static\")\n",
    "    best_static = standardfeeds.latest_available_static(realtime_data.realtime_filename[0][:-4], static_file_path)\n",
    "    static_feed = gk.read_feed(Path(static_file_path, best_static), dist_units=\"km\")\n",
    "    # Add calculated params\n",
    "    bbox = static_feed.compute_bounds()\n",
    "    area_sqkm = spatial.bbox_area(bbox[0], bbox[1], bbox[2], bbox[3])\n",
    "    # All from most active service ID\n",
    "    largest_service_id = static_feed.trips.groupby('service_id')['block_id'].count().idxmax()\n",
    "    trips = static_feed.trips[static_feed.trips['service_id'] == largest_service_id].copy()\n",
    "    # Each trip is a block if no block IDs\n",
    "    if trips['block_id'].nunique() == 0:\n",
    "        no_block_ids = 1\n",
    "        trips['block_id'] = trips['trip_id']\n",
    "    else:\n",
    "        no_block_ids = 0\n",
    "    # Trip distance\n",
    "    shapes = static_feed.geometrize_shapes()\n",
    "    shapes['line_dist_m'] = shapes['geometry'].apply(lambda x: x.length / .00001)\n",
    "    trips = pd.merge(trips, shapes, on='shape_id', how='left')\n",
    "    stop_times = static_feed.stop_times\n",
    "    # Trip active duration\n",
    "    stop_times = stop_times.dropna(subset=['arrival_time', 'departure_time']).copy()\n",
    "    stop_times['start'] = stop_times['arrival_time'].apply(lambda x: int(x.split(\":\")[0])*60 + int(x.split(\":\")[1]))\n",
    "    stop_times['end'] = stop_times['departure_time'].apply(lambda x: int(x.split(\":\")[0])*60 + int(x.split(\":\")[1]))\n",
    "    stop_times = stop_times.groupby('trip_id').agg({'start': 'first', 'end': 'last'})\n",
    "    stop_times['trip_duration_min'] = stop_times['end'] - stop_times['start']\n",
    "    trips = pd.merge(trips, stop_times, on='trip_id', how='left')\n",
    "    # Trip deadhead\n",
    "    trips = trips.sort_values(['block_id', 'start'])\n",
    "    trips['trip_deadhead_min'] = trips.groupby('block_id').apply(lambda x: x['start'].shift(-1) - x['end'], include_groups=False).values\n",
    "    trips['trip_deadhead_min'] = trips['trip_deadhead_min'].fillna(0)\n",
    "    static_data = {\n",
    "        \"provider\": row['provider'],\n",
    "        \"municipality\": row['municipality'],\n",
    "        \"country_code\": row['country_code'],\n",
    "        \"n_realtime_points\": len(realtime_data),\n",
    "        \"n_realtime_trips\": len(pd.unique(realtime_data['trip_id'])),\n",
    "        \"n_static_trips\": len(trips),\n",
    "        \"n_static_blocks\": trips['block_id'].nunique(),\n",
    "        \"no_block_ids\": no_block_ids,\n",
    "        \"service_veh_km\": trips['line_dist_m'].sum() / 1000,\n",
    "        \"mean_block_veh_km\": trips.groupby('block_id')['line_dist_m'].sum().mean() / 1000,\n",
    "        \"mean_block_duration_min\": trips.groupby('block_id')['trip_duration_min'].sum().mean(),\n",
    "        \"mean_block_deadhead_min\": trips.groupby('block_id')['trip_deadhead_min'].sum().mean(),\n",
    "        \"area_sqkm\": area_sqkm,\n",
    "    }\n",
    "    realtime_data['provider'] = row['provider']\n",
    "    all_realtime_data.append(realtime_data)\n",
    "    all_static_data.append(static_data)\n",
    "all_realtime_data = pd.concat(all_realtime_data)\n",
    "all_static_data = pd.DataFrame(all_static_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(10, 18))\n",
    "sns.barplot(x=all_static_data['mean_block_deadhead_min'], y=all_static_data['provider'], ax=axes, order=all_static_data.groupby('provider')['mean_block_deadhead_min'].mean().sort_values(ascending=False).index)\n",
    "print(all_static_data['mean_block_deadhead_min'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(10, 18))\n",
    "sns.barplot(x=all_static_data['mean_block_duration_min'], y=all_static_data['provider'], ax=axes, order=all_static_data.groupby('provider')['mean_block_duration_min'].mean().sort_values(ascending=False).index)\n",
    "print(all_static_data['mean_block_duration_min'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(10, 18))\n",
    "sns.barplot(x=all_static_data['mean_block_veh_km'], y=all_static_data['provider'], ax=axes, order=all_static_data.groupby('provider')['mean_block_veh_km'].mean().sort_values(ascending=False).index)\n",
    "print(all_static_data['mean_block_veh_km'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(10, 18))\n",
    "sns.barplot(x=all_static_data['service_veh_km'], y=all_static_data['provider'], ax=axes, order=all_static_data.groupby('provider')['service_veh_km'].mean().sort_values(ascending=False).index)\n",
    "print(all_static_data['service_veh_km'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(10, 18))\n",
    "sns.barplot(x=all_static_data['n_static_blocks'], y=all_static_data['provider'], ax=axes, order=all_static_data.groupby('provider')['n_static_blocks'].mean().sort_values(ascending=False).index)\n",
    "print(all_static_data['n_static_blocks'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(10, 18))\n",
    "sns.barplot(x=all_static_data['area_sqkm'], y=all_static_data['provider'], ax=axes, order=all_static_data.groupby('provider')['area_sqkm'].mean().sort_values(ascending=False).index)\n",
    "print(all_static_data['area_sqkm'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(10, 18))\n",
    "sns.barplot(x=all_static_data['n_realtime_points'], y=all_static_data['provider'], ax=axes, order=all_static_data.groupby('provider')['n_realtime_points'].mean().sort_values(ascending=False).index)\n",
    "print(all_static_data['n_realtime_points'].describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "valle_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b30fe1de1713ca8e7537eef068b13a2de77ded03f86aab2e80ea73416dd3d704"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
