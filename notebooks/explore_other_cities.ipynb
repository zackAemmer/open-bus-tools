{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import shutil\n",
    "import sys\n",
    "from zoneinfo import ZoneInfo\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import geopandas as gpd\n",
    "import importlib\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import lightning.pytorch as pl\n",
    "import rasterio as rio\n",
    "from rasterio.plot import show\n",
    "import seaborn as sns\n",
    "import shapely\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from torch.utils.data import DataLoader, SequentialSampler, SubsetRandomSampler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from openbustools import plotting, spatial, standardfeeds\n",
    "from openbustools.traveltime import data_loader, model_utils\n",
    "from openbustools.drivecycle import trajectory\n",
    "from openbustools.drivecycle.physics import conditions, energy, vehicle\n",
    "\n",
    "from srai.embedders import GTFS2VecEmbedder\n",
    "from srai.joiners import IntersectionJoiner\n",
    "from srai.loaders import GTFSLoader\n",
    "from srai.loaders.osm_loaders.filters import HEX2VEC_FILTER\n",
    "from srai.neighbourhoods.h3_neighbourhood import H3Neighbourhood\n",
    "from srai.regionalizers import H3Regionalizer, geocode_to_region_gdf\n",
    "from srai.plotting import plot_regions, plot_numeric_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_workers=4\n",
    "    pin_memory=True\n",
    "    accelerator=\"cuda\"\n",
    "else:\n",
    "    num_workers=0\n",
    "    pin_memory=False\n",
    "    accelerator=\"cpu\"\n",
    "\n",
    "logging.getLogger(\"lightning\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"lightning.pytorch.accelerators.cuda\").setLevel(logging.ERROR)\n",
    "\n",
    "model_network = \"kcm\"\n",
    "model_type = \"GRU\"\n",
    "\n",
    "train_days = standardfeeds.get_date_list('2024_01_05', 5)\n",
    "train_days = [x.split(\".\")[0] for x in train_days]\n",
    "test_days = standardfeeds.get_date_list('2024_02_06', 7)\n",
    "test_days = [x.split(\".\")[0] for x in test_days]\n",
    "\n",
    "cleaned_sources = pd.read_csv(Path('..', 'data', 'cleaned_sources.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test base model and heuristic on each city\n",
    "# res_base = {}\n",
    "# res_avg = {}\n",
    "# test_sources = cleaned_sources.iloc[:]\n",
    "# for i, row in test_sources.iterrows():\n",
    "\n",
    "#     # Load base model\n",
    "#     model = model_utils.load_model(\"../logs/\", model_network, model_type, 0)\n",
    "#     model.eval()\n",
    "#     test_data_folders = [f\"../data/other_feeds/{row['uuid']}_realtime/processed\"]\n",
    "#     available_days = [x.name for x in Path(test_data_folders[0], \"training\").glob('*')]\n",
    "#     if test_days[0] not in available_days:\n",
    "#         continue\n",
    "\n",
    "#     # Test inference for city\n",
    "#     print(row['provider'])\n",
    "#     test_dataset = data_loader.NumpyDataset(\n",
    "#         test_data_folders,\n",
    "#         test_days,\n",
    "#         holdout_routes=model.holdout_routes,\n",
    "#         load_in_memory=True,\n",
    "#         config = model.config,\n",
    "#     )\n",
    "#     test_loader = DataLoader(\n",
    "#         test_dataset,\n",
    "#         collate_fn=model.collate_fn,\n",
    "#         batch_size=model.batch_size,\n",
    "#         shuffle=False,\n",
    "#         drop_last=False,\n",
    "#         num_workers=num_workers,\n",
    "#         pin_memory=pin_memory\n",
    "#     )\n",
    "#     trainer = pl.Trainer(\n",
    "#         accelerator=accelerator,\n",
    "#         logger=False,\n",
    "#         inference_mode=True,\n",
    "#         enable_progress_bar=False\n",
    "#     )\n",
    "#     preds_and_labels = trainer.predict(model=model, dataloaders=test_loader)\n",
    "#     preds = np.concatenate([x['preds'] for x in preds_and_labels])\n",
    "#     labels = np.concatenate([x['labels'] for x in preds_and_labels])\n",
    "#     res_base[row['uuid']] = {'preds':preds, 'labels':labels}\n",
    "\n",
    "#     # Load and test heuristic\n",
    "#     model = pickle.load(open(Path(\"../logs/\", model_network, \"AVG-0.pkl\"), 'rb'))\n",
    "#     preds_and_labels = model.predict(test_dataset)\n",
    "#     res_avg[row['uuid']] = {'preds':preds_and_labels['preds'], 'labels':preds_and_labels['labels']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_dir = Path(\"..\", \"results\", model_network, \"multicity_tuning\")\n",
    "# if save_dir.exists():\n",
    "#     shutil.rmtree(save_dir)\n",
    "# save_dir.mkdir(parents=True, exist_ok=True)\n",
    "# with open(save_dir / f\"{model_type}.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(res_base, f)\n",
    "# with open(save_dir / f\"AVG.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(res_avg, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tune, then re-test the base model on increasing number of data samples\n",
    "# n_batches = [1, 10, 100, 500, 1000]\n",
    "# batch_size = 10\n",
    "# res_tuned = {}\n",
    "# test_sources = cleaned_sources.iloc[:]\n",
    "\n",
    "# for i, row in test_sources.iterrows():\n",
    "#     # Load model and check if day has data for given city\n",
    "#     data_folders = [f\"../data/other_feeds/{row['uuid']}_realtime/processed\"]\n",
    "#     available_days = [x.name for x in Path(data_folders[0], \"training\").glob('*')]\n",
    "#     if train_days[0] not in available_days:\n",
    "#         print(f\"Skipping {row['provider']}\")\n",
    "#         continue\n",
    "#     print(row['provider'])\n",
    "#     res_tuned[row['uuid']] = {}\n",
    "#     for j, batch_limit in enumerate(n_batches):\n",
    "#         print(f\"Training with {batch_limit} batches\")\n",
    "#         # Try increasing amounts of training samples per-city\n",
    "#         model = model_utils.load_model(\"../logs/\", model_network, model_type, 0)\n",
    "#         model.train()\n",
    "#         train_dataset = data_loader.NumpyDataset(\n",
    "#             data_folders,\n",
    "#             train_days,\n",
    "#             load_in_memory=True,\n",
    "#             config=model.config\n",
    "#         )\n",
    "#         k_fold = KFold(5, shuffle=True, random_state=42)\n",
    "#         train_idx, val_idx = list(k_fold.split(np.arange(len(train_dataset))))[0]\n",
    "#         train_sampler = SubsetRandomSampler(train_idx)\n",
    "#         val_sampler = SequentialSampler(val_idx)\n",
    "#         train_loader = DataLoader(\n",
    "#             train_dataset,\n",
    "#             collate_fn=model.collate_fn,\n",
    "#             batch_size=batch_size,\n",
    "#             sampler=train_sampler,\n",
    "#             drop_last=True,\n",
    "#             num_workers=num_workers,\n",
    "#             pin_memory=pin_memory,\n",
    "#         )\n",
    "#         val_loader = DataLoader(\n",
    "#             train_dataset,\n",
    "#             collate_fn=model.collate_fn,\n",
    "#             batch_size=batch_size,\n",
    "#             sampler=val_sampler,\n",
    "#             drop_last=True,\n",
    "#             num_workers=num_workers,\n",
    "#             pin_memory=pin_memory,\n",
    "#         )\n",
    "#         trainer = pl.Trainer(\n",
    "#             check_val_every_n_epoch=1,\n",
    "#             max_epochs=100,\n",
    "#             accelerator=accelerator,\n",
    "#             callbacks=[EarlyStopping(monitor=f\"valid_loss\", min_delta=.0001, patience=3)],\n",
    "#             limit_train_batches=batch_limit,\n",
    "#             enable_progress_bar=False\n",
    "#         )\n",
    "#         trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "#         # Test after fine-tuning\n",
    "#         model.eval()\n",
    "#         test_dataset = data_loader.NumpyDataset(\n",
    "#             data_folders,\n",
    "#             test_days,\n",
    "#             load_in_memory=True,\n",
    "#             config=model.config\n",
    "#         )\n",
    "#         test_loader = DataLoader(\n",
    "#             test_dataset,\n",
    "#             collate_fn=model.collate_fn,\n",
    "#             batch_size=batch_size,\n",
    "#             shuffle=False,\n",
    "#             drop_last=False,\n",
    "#             num_workers=num_workers,\n",
    "#             pin_memory=pin_memory\n",
    "#         )\n",
    "#         trainer = pl.Trainer(\n",
    "#             accelerator=accelerator,\n",
    "#             logger=False,\n",
    "#             inference_mode=True,\n",
    "#             enable_progress_bar=False\n",
    "#         )\n",
    "#         preds_and_labels = trainer.predict(model=model, dataloaders=test_loader)\n",
    "#         preds = np.concatenate([x['preds'] for x in preds_and_labels])\n",
    "#         labels = np.concatenate([x['labels'] for x in preds_and_labels])\n",
    "#         res_tuned[row['uuid']][f\"{batch_limit}_batches\"] = {'preds':preds, 'labels':labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_dir = Path(\"..\", \"results\", model_network, \"multicity_tuning\")\n",
    "# save_dir.mkdir(parents=True, exist_ok=True)\n",
    "# with open(save_dir / f\"{model_type}_TUNED.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(res_tuned, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_base = pickle.load(open(Path(\"..\", \"results\", model_network, \"multicity_tuning\", f\"{model_type}.pkl\"), \"rb\"))\n",
    "res_avg = pickle.load(open(Path(\"..\", \"results\", model_network, \"multicity_tuning\", \"AVG.pkl\"), \"rb\"))\n",
    "res_tuned = pickle.load(open(Path(\"..\", \"results\", model_network, \"multicity_tuning\", f\"{model_type}_TUNED.pkl\"), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}\n",
    "for k_uuid, res_uuid in res_base.items():\n",
    "    metrics[k_uuid] = model_utils.performance_metrics(res_uuid['labels'], res_uuid['preds'])\n",
    "metrics_base_df = pd.DataFrame(metrics).T\n",
    "metrics_base_df.index.names = ['uuid']\n",
    "metrics_base_df['n_batches'] = '0_batches'\n",
    "metrics_base_df = metrics_base_df.reset_index()\n",
    "metrics_base_df = pd.merge(metrics_base_df, cleaned_sources, on='uuid')\n",
    "metrics_base_df['experiment'] = 'not_tuned'\n",
    "metrics_base_df[['uuid','provider','n_batches','experiment','mae', 'mape', 'rmse', 'ex_var', 'r_score']].sort_values(['provider', 'n_batches']).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}\n",
    "for k_uuid, res_uuid in res_tuned.items():\n",
    "    for k_n_batches, res_n_batches in res_uuid.items():\n",
    "        metrics[(k_uuid, k_n_batches)] = model_utils.performance_metrics(res_n_batches['labels'], res_n_batches['preds'])\n",
    "metrics_tuned_df = pd.DataFrame(metrics).T\n",
    "metrics_tuned_df.index.names = ['uuid', 'n_batches']\n",
    "metrics_tuned_df = metrics_tuned_df.reset_index()\n",
    "metrics_tuned_df = pd.merge(metrics_tuned_df, cleaned_sources, on='uuid')\n",
    "metrics_tuned_df['experiment'] = 'tuned'\n",
    "metrics_tuned_df[['uuid','provider','n_batches','experiment','mae', 'mape', 'rmse', 'ex_var', 'r_score']].sort_values(['provider', 'n_batches']).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = pd.concat([metrics_base_df, metrics_tuned_df]).sort_values(['provider', 'n_batches'])\n",
    "all_metrics['n_batches'] = pd.Categorical(all_metrics['n_batches'], ['0_batches','1_batches','10_batches','100_batches','500_batches','1000_batches','avg'])\n",
    "all_metrics['Tuning Sample Size'] = all_metrics['n_batches'].replace({'0_batches': 'No Tuning', '1_batches':'10 Samples', '10_batches':'100 Samples', '100_batches':'1,000 Samples', '500_batches':'5,000 Samples', '1000_batches':'10,000 Samples', 'avg':'Heuristic'})\n",
    "all_metrics['MAPE'] = all_metrics['mape']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,1)\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(8)\n",
    "sns.boxplot(ax=axes, data=all_metrics, x='MAPE', hue='Tuning Sample Size', palette=plotting.PALETTE)\n",
    "axes.set_xlim([.08, .3])\n",
    "axes.set_title('Tuned Performance for 33 International Cities')\n",
    "axes.legend(handles=axes.get_legend_handles_labels()[0], loc='upper left', ncol=1)\n",
    "axes.set_xticklabels(['10%', '12.5%', '15%', '17.5%', '20%', '22.5%', '25%', '27.5%', '30%'])\n",
    "fig.savefig(\"../plots/multicity_generalization.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}\n",
    "for k_uuid, res_uuid in res_avg.items():\n",
    "    metrics[k_uuid] = model_utils.performance_metrics(res_uuid['labels'], res_uuid['preds'])\n",
    "metrics_avg_df = pd.DataFrame(metrics).T\n",
    "metrics_avg_df.index.names = ['uuid']\n",
    "metrics_avg_df['n_batches'] = 'avg'\n",
    "metrics_avg_df = metrics_avg_df.reset_index()\n",
    "metrics_avg_df = pd.merge(metrics_avg_df, cleaned_sources, on='uuid')\n",
    "metrics_avg_df['experiment'] = 'heuristic'\n",
    "metrics_avg_df[['uuid','provider','n_batches','experiment','mae', 'mape', 'rmse', 'ex_var', 'r_score']].sort_values(['provider', 'n_batches']).head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
