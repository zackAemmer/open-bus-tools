{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "from zoneinfo import ZoneInfo\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import geopandas as gpd\n",
    "import importlib\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import lightning.pytorch as pl\n",
    "import rasterio as rio\n",
    "from rasterio.plot import show\n",
    "import seaborn as sns\n",
    "import shapely\n",
    "import statsmodels.api as sm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from openbustools import plotting, spatial, standardfeeds\n",
    "from openbustools.traveltime import data_loader, model_utils\n",
    "from openbustools.drivecycle import trajectory\n",
    "from openbustools.drivecycle.physics import conditions, energy, vehicle\n",
    "\n",
    "from srai.embedders import GTFS2VecEmbedder\n",
    "from srai.joiners import IntersectionJoiner\n",
    "from srai.loaders import GTFSLoader\n",
    "from srai.loaders.osm_loaders.filters import HEX2VEC_FILTER\n",
    "from srai.neighbourhoods.h3_neighbourhood import H3Neighbourhood\n",
    "from srai.regionalizers import H3Regionalizer, geocode_to_region_gdf\n",
    "from srai.plotting import plot_regions, plot_numeric_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_workers=4\n",
    "    pin_memory=True\n",
    "    accelerator=\"cuda\"\n",
    "else:\n",
    "    num_workers=0\n",
    "    pin_memory=False\n",
    "    accelerator=\"cpu\"\n",
    "\n",
    "logging.getLogger(\"lightning\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"lightning.pytorch.accelerators.cuda\").setLevel(logging.ERROR)\n",
    "\n",
    "train_days = standardfeeds.get_date_list('2024_01_04', 1)\n",
    "train_days = [x.split(\".\")[0] for x in train_days]\n",
    "test_days = standardfeeds.get_date_list('2024_01_05', 1)\n",
    "test_days = [x.split(\".\")[0] for x in test_days]\n",
    "\n",
    "cleaned_sources = pd.read_csv(Path('..', 'data', 'cleaned_sources.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model on each city\n",
    "res = {}\n",
    "test_sources = cleaned_sources.iloc[:]\n",
    "for i, row in test_sources.iterrows():\n",
    "    # Load model and check if day has data for given city\n",
    "    model = model_utils.load_model(\"../logs/\", \"kcm\", \"GRU\", 0)\n",
    "    model.eval()\n",
    "    test_data_folders = [f\"../data/other_feeds/{row['uuid']}_realtime/processed\"]\n",
    "    available_days = [x.name for x in Path(test_data_folders[0], \"training\").glob('*')]\n",
    "    if test_days[0] not in available_days:\n",
    "        continue\n",
    "    # Run inference for city\n",
    "    print(row['provider'])\n",
    "    test_dataset = data_loader.NumpyDataset(\n",
    "        test_data_folders,\n",
    "        test_days,\n",
    "        holdout_routes=model.holdout_routes,\n",
    "        load_in_memory=True,\n",
    "        config = model.config,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        collate_fn=model.collate_fn,\n",
    "        batch_size=model.batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory\n",
    "    )\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=accelerator,\n",
    "        logger=False,\n",
    "        inference_mode=True\n",
    "    )\n",
    "    preds_and_labels = trainer.predict(model=model, dataloaders=test_loader)\n",
    "    preds = np.concatenate([x['preds'] for x in preds_and_labels])\n",
    "    labels = np.concatenate([x['labels'] for x in preds_and_labels])\n",
    "    res[row['uuid']] = {'preds':preds, 'labels':labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune, then re-test\n",
    "n_batches = 1\n",
    "res_tuned = {}\n",
    "test_sources = cleaned_sources.iloc[:]\n",
    "for i, row in test_sources.iterrows():\n",
    "    # Load model and check if day has data for given city\n",
    "    data_folders = [f\"../data/other_feeds/{row['uuid']}_realtime/processed\"]\n",
    "    available_days = [x.name for x in Path(data_folders[0], \"training\").glob('*')]\n",
    "    if train_days[0] not in available_days:\n",
    "        continue\n",
    "    print(row['provider'])\n",
    "    res_tuned[row['uuid']] = {}\n",
    "    # Try increasing amounts of training samples per-city\n",
    "    model = model_utils.load_model(\"../logs/\", \"kcm\", \"GRU\", 0)\n",
    "    model.eval()\n",
    "    train_dataset = data_loader.NumpyDataset(\n",
    "        data_folders,\n",
    "        train_days,\n",
    "        load_in_memory=True,\n",
    "        config = model.config\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        collate_fn=model.collate_fn,\n",
    "        batch_size=model.batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory\n",
    "    )\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=10,\n",
    "        limit_train_batches=n_batches,\n",
    "        accelerator=accelerator\n",
    "    )\n",
    "    trainer.fit(model=model, train_dataloaders=train_loader)\n",
    "    # Test after fine-tuning\n",
    "    test_dataset = data_loader.NumpyDataset(\n",
    "        data_folders,\n",
    "        test_days,\n",
    "        load_in_memory=True,\n",
    "        config = model.config\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        collate_fn=model.collate_fn,\n",
    "        batch_size=model.batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory\n",
    "    )\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=accelerator,\n",
    "        logger=False,\n",
    "        inference_mode=True\n",
    "    )\n",
    "    preds_and_labels = trainer.predict(model=model, dataloaders=test_loader)\n",
    "    preds = np.concatenate([x['preds'] for x in preds_and_labels])\n",
    "    labels = np.concatenate([x['labels'] for x in preds_and_labels])\n",
    "    res_tuned[row['uuid']][f\"{n_batches}_batches\"] = {'preds':preds, 'labels':labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}\n",
    "for key, value in res.items():\n",
    "    metrics[key] = model_utils.performance_metrics(value['labels'], value['preds'])\n",
    "metrics_df = pd.DataFrame(metrics).T\n",
    "metrics_df.index.name = 'uuid'\n",
    "metrics_df = metrics_df.reset_index()\n",
    "metrics_df = pd.merge(metrics_df, cleaned_sources, on='uuid')\n",
    "metrics_df['experiment'] = 'not-tuned'\n",
    "metrics_df['n_batches'] = '0_batches'\n",
    "metrics_df[['provider','n_batches','experiment','mae', 'mape', 'rmse', 'ex_var', 'r_score']].sort_values(['provider', 'n_batches']).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}\n",
    "for k_uuid, res_uuid in res_tuned.items():\n",
    "    for k_n_batches, res_n_batches in res_uuid.items():\n",
    "        metrics[(k_uuid, k_n_batches)] = model_utils.performance_metrics(res_n_batches['labels'], res_n_batches['preds'])\n",
    "metrics_tuned_df = pd.DataFrame(metrics).T\n",
    "metrics_tuned_df.index.names = ['uuid', 'n_batches']\n",
    "metrics_tuned_df = metrics_tuned_df.reset_index()\n",
    "metrics_tuned_df = pd.merge(metrics_tuned_df, cleaned_sources, on='uuid')\n",
    "metrics_tuned_df['experiment'] = 'tuned'\n",
    "metrics_tuned_df[['provider','n_batches','experiment','mae', 'mape', 'rmse', 'ex_var', 'r_score']].sort_values(['provider', 'n_batches']).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = pd.concat([metrics_df, metrics_tuned_df], axis=0)\n",
    "plot_df = plot_df[plot_df['n_batches'].isin(['0_batches', '1_batches', '2_batches', '4_batches'])]\n",
    "plot_df['Tuning Sample Size'] = plot_df['n_batches'].replace({'0_batches': 'No Tuning', '1_batches': '1000 Samples', '2_batches': '2000 Samples', '4_batches': '4000 Samples'})\n",
    "plot_df['MAPE'] = plot_df['mape']\n",
    "sns.histplot(plot_df, x='MAPE', hue='Tuning Sample Size', bins=50, kde=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
